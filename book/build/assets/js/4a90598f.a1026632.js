"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[601],{4286(e,o,n){n.r(o),n.d(o,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var i=n(4848),t=n(8453);const a={title:"Module 4 - Vision-Language-Action (VLA)",sidebar_label:"VLA",sidebar_position:5,description:"Convergence of LLMs and Robotics for autonomous behavior"},s="Module 4: Vision-Language-Action (VLA)",r={id:"module-4-vla/index",title:"Module 4 - Vision-Language-Action (VLA)",description:"Convergence of LLMs and Robotics for autonomous behavior",source:"@site/docs/module-4-vla/index.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/",permalink:"/docs/module-4-vla/",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/physical-ai-book/tree/main/docs/module-4-vla/index.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Module 4 - Vision-Language-Action (VLA)",sidebar_label:"VLA",sidebar_position:5,description:"Convergence of LLMs and Robotics for autonomous behavior"},sidebar:"tutorialSidebar",previous:{title:"Nav2 Humanoid",permalink:"/docs/module-3-ai-brain/nav2-humanoid"},next:{title:"Voice-to-Action",permalink:"/docs/module-4-vla/voice-action"}},l={},c=[{value:"Topics Covered",id:"topics-covered",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"References",id:"references",level:2}];function d(e){const o={h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,i.jsx)(o.p,{children:"This module explores the convergence of LLMs and Robotics for autonomous behavior. You'll learn how to integrate multimodal AI pipelines for humanoid robots that can respond to voice commands."}),"\n",(0,i.jsx)(o.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Voice-to-action: OpenAI Whisper integration"}),"\n",(0,i.jsx)(o.li,{children:"Cognitive Planning: LLMs translate natural language into ROS 2 actions"}),"\n",(0,i.jsx)(o.li,{children:"Capstone Project: Autonomous Humanoid (voice command \u2192 perception \u2192 action \u2192 manipulation)"}),"\n"]}),"\n",(0,i.jsx)(o.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Integrate multimodal AI pipelines for autonomous humanoid behavior"}),"\n"]}),"\n",(0,i.jsx)(o.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsx)(o.li,{children:"Placeholder for references"}),"\n"]})]})}function u(e={}){const{wrapper:o}={...(0,t.R)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,o,n){n.d(o,{R:()=>s,x:()=>r});var i=n(6540);const t={},a=i.createContext(t);function s(e){const o=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function r(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:o},e.children)}}}]);