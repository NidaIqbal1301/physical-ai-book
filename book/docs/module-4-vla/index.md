---
title: Module 4 - Vision-Language-Action (VLA)
sidebar_label: VLA
sidebar_position: 5
description: Convergence of LLMs and Robotics for autonomous behavior
---

# Module 4: Vision-Language-Action (VLA)

This module explores the convergence of LLMs and Robotics for autonomous behavior. You'll learn how to integrate multimodal AI pipelines for humanoid robots that can respond to voice commands.

## Topics Covered

- Voice-to-action: OpenAI Whisper integration
- Cognitive Planning: LLMs translate natural language into ROS 2 actions
- Capstone Project: Autonomous Humanoid (voice command → perception → action → manipulation)

## Learning Objectives

- Integrate multimodal AI pipelines for autonomous humanoid behavior

## References

- Placeholder for references